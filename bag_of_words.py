# -*- coding: utf-8 -*-
"""Bag of Words.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/175hMIln3W0mo4CkVdohM_ngkACIW6x7b

#Corpus

Sua  tarefa  será  transformar  um  conjunto  de  5  sites,  sobre  o  tema  de  processamento  de 
linguagem natural em um conjunto de cinco listas distintas de sentenças. Ou seja, você fará uma função 
que, usando a biblioteca Beautifull Soap, faça a requisição de uma url, e extrai todas as sentenças desta 
url. Duas condições são importantes:  
>  a) A página web (url) deve apontar para uma página web em inglês contendo, não menos que 1000 palavras.  

> b) O texto desta página deverá ser transformado em um array de senteças.  
 
Para separar as sentenças você pode usar os sinais de pontuação ou as funções da biblibioteca 
Spacy.
"""

import requests
import spacy
import bs4
from bs4 import BeautifulSoup

def adiciona_site(site, lista):

  page = requests.get(site)
  if  not 200==page.status_code:
    print("Site: "+site+" não abriu")
    print(page.status_code)
    return lista
  soap = BeautifulSoup(page.content, 'html.parser')

  spaci = spacy.load('en_core_web_sm')

  
  for i in soap.find_all('p'):
    espaco = spaci(i.get_text())
    for j in espaco.sents:
      lista.append(j)
  return lista

sentencas = []
sentencas1 = adiciona_site("https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP", [])
sentencas2 = adiciona_site("https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html#world", [])
sentencas3 = adiciona_site("https://monkeylearn.com/natural-language-processing/", [])
sentencas4 = adiciona_site("https://hbr.org/2022/04/the-power-of-natural-language-processing", [])
sentencas5 = adiciona_site("https://www.oracle.com/artificial-intelligence/what-is-natural-language-processing/", [])
sentencas.append(sentencas1)
sentencas.append(sentencas2)
sentencas.append(sentencas3)
sentencas.append(sentencas4)
sentencas.append(sentencas5)
print(sentencas)

"""#Bag of Words

Sua tarefa será  gerar a matriz termo documento, dos documentos recuperados da internet e 
imprimir esta matriz na tela. Para tanto: 
>a) Considere que todas as listas de sentenças devem ser transformadas em listas de vetores, 
onde cada item será uma das palavras da sentença. 

>b) Todos  os  vetores  devem  ser  unidos  em  um  corpus  único  formando  uma  lista  de  vetores, 
onde cada item será um lexema. 

>c) Este único corpus será usado para gerar o vocabulário. 

>d) O  resultado  esperado  será  uma  matriz  termo  documento  criada  a  partir  da  aplicação  da 
técnica bag of Words em todo o corpus.  
"""

def adiciona_palavra_bag (sentencas, bag, doc,tamanho_doc,x,y):
  palavra = sentencas[x][y]
  #removedor de espaços  ®
  if palavra.dep_ == "SPACE" or palavra.dep_ == "dep" or palavra.dep_ == "punct":
    excecoes = ["referred","hinges","have","so","am","recall","'d"]
    if not palavra.text in excecoes:
      return bag
  achei = False
  for i in bag:
    if i[0].text == palavra.text:
      achei = True
      i[doc] += 1

  if achei:
    return bag
  bag.append([palavra])
  for i in range(tamanho_doc):
    bag[len(bag)-1].append(0)
  bag[len(bag)-1][doc] = 1
  return bag

corpus = []
doc = -1
for i in sentencas:
  for j in i:
    doc += 1
    corpus.append([])
    for k in j:
      corpus[doc].append(k)

bag = []
#spacy.displacy.render(sentencas1[9], style="dep", jupyter = True)
for i in range(len(corpus)):
  for j in range(len(corpus[i])):
    bag = adiciona_palavra_bag(corpus,bag,i+1,len(corpus),i,j)

# bag muito grande para printar inteiro

print(len(bag))
for i in range(400):
  print(bag[i])